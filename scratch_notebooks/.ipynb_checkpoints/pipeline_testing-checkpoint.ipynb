{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1373a9d1-1924-43bd-8ac2-763627d98cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gautom/anaconda3/envs/lavis/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5a32f14e944bda9bb69407fc775218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing model...\n"
     ]
    }
   ],
   "source": [
    "from blip_quantizer import BlipQuantizer, QuantConfig, ModelPart, LayerGroup, LayerType\n",
    "from quant_functions import uniform_quantization\n",
    "import torch\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from datasets import COCODataset\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import print_model_structure\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "quantizer = BlipQuantizer(model)\n",
    "configs = [\n",
    "    QuantConfig(ModelPart.VIT, LayerGroup.MIDDLE, LayerType.MLP, \n",
    "                uniform_quantization, num_bits=8),\n",
    "    QuantConfig(ModelPart.QFORMER, LayerGroup.MIDDLE, LayerType.MLP, \n",
    "                uniform_quantization, num_bits=4),\n",
    "    QuantConfig(ModelPart.LLM, LayerGroup.MIDDLE, LayerType.MLP, \n",
    "                uniform_quantization, num_bits=4)\n",
    "]\n",
    "\n",
    "\n",
    "print(\"Quantizing model...\")\n",
    "quantizer.apply_quantization(configs)\n",
    "\n",
    "# print_model_structure(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e6a6472-a6a8-4eaa-a8c3-07fcb2ffe50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "Starting evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                  | 0/1000 [00:00<?, ?it/s]/home/gautom/anaconda3/envs/lavis/lib/python3.8/site-packages/transformers/generation/utils.py:1133: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████████████████████████████████████| 1000/1000 [03:31<00:00,  4.73it/s]\n",
      "PTBTokenizer tokenized 61766 tokens at 310803.25 tokens per second.\n",
      "PTBTokenizer tokenized 5489 tokens at 55702.28 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO CIDEr score: 0.6399606432560109\n",
      "\n",
      "Example predictions:\n",
      "Image ID: 397133\n",
      "Prediction: a woman in a kitchen\n",
      "References: ['A man is in a kitchen making pizzas.', 'Man in apron standing on front of oven with pans and bakeware', 'A baker is working in the kitchen rolling dough.', 'A person standing by a stove in a kitchen.', 'A table with pies being made and a person standing near a wall with pots and pans hanging on the wall.']\n",
      "Individual CIDEr score: 0.7323661412056455\n",
      "\n",
      "Image ID: 37777\n",
      "Prediction: kitchen\n",
      "References: ['The dining table near the kitchen has a bowl of fruit on it.', 'A small kitchen has various appliances and a table.', 'The kitchen is clean and ready for us to see.', 'A kitchen and dining area decorated in white.', 'A kitchen that has a bowl of fruit on the table.']\n",
      "Individual CIDEr score: 0.3339097712588217\n",
      "\n",
      "Image ID: 252219\n",
      "Prediction: man and woman walking down the street\n",
      "References: ['a person with a shopping cart on a city street ', 'City dwellers walk by as a homeless man begs for cash.', 'People walking past a homeless man begging on a city street', 'a homeless man holding a cup and standing next to a shopping cart on a street', 'People are walking on the street by a homeless person.']\n",
      "Individual CIDEr score: 0.34252773668736514\n",
      "\n",
      "Image ID: 87038\n",
      "Prediction: skateboarders in a skate park\n",
      "References: ['A person on a skateboard and bike at a skate park.', 'A man on a skateboard performs a trick at the skate park', 'A skateboarder jumps into the air as he performs a skateboard trick.', 'Athletes performing tricks on a BMX bicycle and a skateboard.', 'a man falls off his skateboard in a skate park.']\n",
      "Individual CIDEr score: 0.8217013652751712\n",
      "\n",
      "Image ID: 174482\n",
      "Prediction: bicycle parked on the street\n",
      "References: ['a blue bike parked on a side walk ', 'A bicycle is chained to a fixture on a city street', 'A blue bicycle sits on a sidewalk near a street.', 'A bicycle is locked up to a post', 'a bike sits parked next to a street ']\n",
      "Individual CIDEr score: 0.8107375806026661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from evaluation_pipeline import EvaluationPipeline\n",
    "\n",
    "# from evaluation_pipeline import EvaluationPipeline\n",
    "coco_dataset = COCODataset(ann_file='./data/coco/annotations/captions_val2017.json',\n",
    "                           img_dir='./data/coco/val2017')\n",
    "\n",
    "model_name = \"Salesforce/blip2-opt-2.7b\"\n",
    "processor = Blip2Processor.from_pretrained(model_name)\n",
    "\n",
    "# Create evaluator\n",
    "evaluator = EvaluationPipeline(model, processor, device)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Starting evaluation...\")\n",
    "coco_results = evaluator.evaluate(coco_dataset, task='image_captioning', max_samples=1000)\n",
    "\n",
    "# Save results\n",
    "evaluator.save_results(coco_results, './results/coco_quantized_evaluation.json')\n",
    "\n",
    "# Print overall CIDEr score\n",
    "print(f\"COCO CIDEr score: {coco_results['overall_cider']}\")\n",
    "\n",
    "# Print a few example predictions\n",
    "print(\"\\nExample predictions:\")\n",
    "for i in range(5):  # Print first 5 predictions\n",
    "    print(f\"Image ID: {coco_results['predictions'][i]['image_id']}\")\n",
    "    print(f\"Prediction: {coco_results['predictions'][i]['caption']}\")\n",
    "    print(f\"References: {coco_results['references'][i]}\")\n",
    "    print(f\"Individual CIDEr score: {coco_results['individual_cider'][i]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bf557c0-17e3-49b4-b89a-208ba781718f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "Starting inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 20/20 [00:03<00:00,  5.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 1235 tokens at 13214.43 tokens per second.\n",
      "PTBTokenizer tokenized 92 tokens at 1219.63 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing scores...\n",
      "Computing ['Bleu_1', 'Bleu_2', 'Bleu_3', 'Bleu_4'] score...\n",
      "{'testlen': 73, 'reflen': 173, 'guess': [73, 53, 35, 20], 'correct': [63, 25, 10, 2]}\n",
      "ratio: 0.42196531791663605\n",
      "Computing METEOR score...\n",
      "Computing ROUGE_L score...\n",
      "Computing CIDEr score...\n",
      "Computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/home/gautom/Documents/lavis/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\n",
      "WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.5 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.1 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Threads( StanfordCoreNLP ) [0.561 seconds]\n",
      "Warning: Nashorn engine is planned to be removed from a future JDK release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 5.914 s\n",
      "Bleu_1: 0.21932782985573562\n",
      "Bleu_2: 0.162150004990946\n",
      "Bleu_3: 0.12405491633945606\n",
      "Bleu_4: 0.0834602148134694\n",
      "METEOR: 0.15735645084223648\n",
      "ROUGE_L: 0.35939849127554524\n",
      "CIDEr: 0.6174471337969347\n",
      "SPICE: 0.15014796021888632\n",
      "\n",
      "Example predictions:\n",
      "Image ID: 397133\n",
      "Prediction: a woman in a kitchen\n",
      "References: ['A man is in a kitchen making pizzas.', 'Man in apron standing on front of oven with pans and bakeware', 'A baker is working in the kitchen rolling dough.', 'A person standing by a stove in a kitchen.', 'A table with pies being made and a person standing near a wall with pots and pans hanging on the wall.']\n",
      "Individual CIDEr score: 0.689941342710598\n",
      "\n",
      "Image ID: 37777\n",
      "Prediction: kitchen\n",
      "References: ['The dining table near the kitchen has a bowl of fruit on it.', 'A small kitchen has various appliances and a table.', 'The kitchen is clean and ready for us to see.', 'A kitchen and dining area decorated in white.', 'A kitchen that has a bowl of fruit on the table.']\n",
      "Individual CIDEr score: 0.3408877490432992\n",
      "\n",
      "Image ID: 252219\n",
      "Prediction: man and woman walking down the street\n",
      "References: ['a person with a shopping cart on a city street ', 'City dwellers walk by as a homeless man begs for cash.', 'People walking past a homeless man begging on a city street', 'a homeless man holding a cup and standing next to a shopping cart on a street', 'People are walking on the street by a homeless person.']\n",
      "Individual CIDEr score: 0.3561292611420875\n",
      "\n",
      "Image ID: 87038\n",
      "Prediction: skateboarders in a skate park\n",
      "References: ['A person on a skateboard and bike at a skate park.', 'A man on a skateboard performs a trick at the skate park', 'A skateboarder jumps into the air as he performs a skateboard trick.', 'Athletes performing tricks on a BMX bicycle and a skateboard.', 'a man falls off his skateboard in a skate park.']\n",
      "Individual CIDEr score: 1.0549868345761224\n",
      "\n",
      "Image ID: 174482\n",
      "Prediction: bicycle parked on the street\n",
      "References: ['a blue bike parked on a side walk ', 'A bicycle is chained to a fixture on a city street', 'A blue bicycle sits on a sidewalk near a street.', 'A bicycle is locked up to a post', 'a bike sits parked next to a street ']\n",
      "Individual CIDEr score: 0.714563478613875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from inference_pipeline import InferencePipeline\n",
    "\n",
    "model_name = \"Salesforce/blip2-opt-2.7b\"\n",
    "processor = Blip2Processor.from_pretrained(model_name)\n",
    "\n",
    "# Set up the model, processor, and dataset as before\n",
    "model_name = \"Salesforce/blip2-opt-2.7b\"\n",
    "coco_dataset = COCODataset(ann_file='./data/coco/annotations/captions_val2017.json',\n",
    "                           img_dir='./data/coco/val2017')\n",
    "\n",
    "# Run inference\n",
    "inferencer = InferencePipeline(model, processor, device)\n",
    "print(\"Starting inference...\")\n",
    "results = inferencer.run_inference(coco_dataset, task='image_captioning', max_samples=20)\n",
    "inferencer.save_results(results, './results/coco_quantized_inference.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acbebad8-8e91-4bc9-9de3-9854335d2f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 1235 tokens at 13131.68 tokens per second.\n",
      "PTBTokenizer tokenized 92 tokens at 1169.08 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing scores...\n",
      "Computing METEOR score...\n",
      "Computing CIDEr score...\n",
      "METEOR: 0.15735645084223648\n",
      "CIDEr: 0.6174471337969347\n",
      "\n",
      "Example predictions:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExample predictions:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):  \u001b[38;5;66;03m# Print first 5 predictions\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m][i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m][i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReferences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreferences\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "from scoring_pipeline import ScoringPipeline\n",
    "\n",
    "# Compute scores\n",
    "scorer = ScoringPipeline()\n",
    "loaded_results = scorer.load_results('./results/coco_quantized_inference.json')\n",
    "scores = scorer.compute_scores(loaded_results, task='image_captioning')\n",
    "\n",
    "# Print scores\n",
    "for metric, score in scores.items():\n",
    "    if not metric.endswith('_per_caption'):\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44456709-defa-4fc0-839c-502c8480c032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "model.to('cpu')\n",
    "del model, evaluator\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44118fa-7186-4f78-93bc-f215dae898fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
