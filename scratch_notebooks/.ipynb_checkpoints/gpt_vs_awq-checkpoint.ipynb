{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a20d69b7-a03f-468e-b097-d81f5ac9b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def quantize_model(model, num_bits, method='uniform', block_size=128, lambda_param=1e-5):\n",
    "    def quantize_uniform(tensor, num_bits):\n",
    "        qmin, qmax = 0, 2**num_bits - 1\n",
    "        scale = (tensor.max() - tensor.min()) / (qmax - qmin)\n",
    "        zero_point = qmin - torch.round(tensor.min() / scale)\n",
    "        \n",
    "        quantized = torch.clamp(torch.round(tensor / scale + zero_point), qmin, qmax)\n",
    "        dequantized = (quantized - zero_point) * scale\n",
    "        \n",
    "        return dequantized\n",
    "\n",
    "    def quant(x, num_bits):\n",
    "        return quantize_uniform(x, num_bits)\n",
    "\n",
    "    def gptq(W, H_inv, num_bits, block_size):\n",
    "        d_row, d_col = W.shape\n",
    "        Q = torch.zeros_like(W)\n",
    "        E = torch.zeros(d_row, block_size, device=W.device)\n",
    "\n",
    "        for i in range(0, d_col, block_size):\n",
    "            curr_block_size = min(block_size, d_col - i)\n",
    "            H_inv_block = H_inv[i:i+curr_block_size, i:i+curr_block_size]\n",
    "            \n",
    "            for j in range(i, i + curr_block_size):\n",
    "                Q[:, j] = quant(W[:, j], num_bits)\n",
    "                E[:, j-i] = W[:, j] - Q[:, j]\n",
    "                W[:, j:i+curr_block_size] -= E[:, j-i].unsqueeze(1) @ H_inv_block[j-i:j-i+1, :]\n",
    "\n",
    "            if i + curr_block_size < d_col:\n",
    "                W[:, i+curr_block_size:] -= E @ H_inv[i:i+curr_block_size, i+curr_block_size:]\n",
    "\n",
    "        return Q\n",
    "\n",
    "    def compute_hessian(X, lambda_param):\n",
    "        X = X.T\n",
    "        H = 2 * X @ X.T\n",
    "        H_reg = H + lambda_param * torch.eye(H.shape[0], device=H.device)\n",
    "        return torch.inverse(H_reg)\n",
    "\n",
    "    def quantize_layer_gptq(layer, X):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            H_inv = compute_hessian(X, lambda_param)\n",
    "            layer.weight.data = gptq(layer.weight.data, H_inv, num_bits, block_size)\n",
    "            if layer.bias is not None:\n",
    "                layer.bias.data = quantize_uniform(layer.bias.data, num_bits)\n",
    "        return layer\n",
    "\n",
    "    def quantize_layer_uniform(layer):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            layer.weight.data = quantize_uniform(layer.weight.data, num_bits)\n",
    "            if layer.bias is not None:\n",
    "                layer.bias.data = quantize_uniform(layer.bias.data, num_bits)\n",
    "        return layer\n",
    "\n",
    "    if method == 'uniform':\n",
    "        return model.apply(quantize_layer_uniform)\n",
    "    elif method == 'gptq':\n",
    "        device = next(model.parameters()).device\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                in_features = module.in_features\n",
    "                # Generate random input for this layer\n",
    "                X = torch.randn(100, in_features, device=device)  # Increased sample size\n",
    "                quantize_layer_gptq(module, X)\n",
    "        return model\n",
    "    elif method == 'awq':\n",
    "        raise NotImplementedError(\"AWQ quantization not implemented\")\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported quantization method\")\n",
    "\n",
    "# Example usage:\n",
    "# quantized_model = quantize_model(model, num_bits=8, method='gptq', block_size=128, lambda_param=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57298cea-d0f7-4fd7-b236-753cf8c6a794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546d546e5c0546a3a206fc3cd74fc43f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.08s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from datasets import COCODataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load BLIP-2 model and processor\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "# Load COCO dataset\n",
    "coco_dataset = COCODataset(ann_file='./data/coco/annotations/captions_val2017.json',\n",
    "                           img_dir='./data/coco/val2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "828f7498-7dac-4dbb-b003-956753f4d3db",
   "metadata": {},
   "outputs": [
    {
     "ename": "_LinAlgError",
     "evalue": "linalg.cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 43 is not positive-definite).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_LinAlgError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m quantized_model \u001b[38;5;241m=\u001b[39m \u001b[43mquantize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgptq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_param\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 66\u001b[0m, in \u001b[0;36mquantize_model\u001b[0;34m(model, num_bits, method, block_size, lambda_param)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[38;5;66;03m# Generate random input for this layer\u001b[39;00m\n\u001b[1;32m     65\u001b[0m             X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, in_features, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 66\u001b[0m             \u001b[43mquantize_layer_gptq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mawq\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "Cell \u001b[0;32mIn[9], line 45\u001b[0m, in \u001b[0;36mquantize_model.<locals>.quantize_layer_gptq\u001b[0;34m(layer, X)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, nn\u001b[38;5;241m.\u001b[39mLinear):\n\u001b[1;32m     44\u001b[0m     H_inv \u001b[38;5;241m=\u001b[39m compute_hessian(X, lambda_param)\n\u001b[0;32m---> 45\u001b[0m     layer\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mgptq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH_inv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m         layer\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m quantize_uniform(layer\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata, num_bits)\n",
      "Cell \u001b[0;32mIn[9], line 23\u001b[0m, in \u001b[0;36mquantize_model.<locals>.gptq\u001b[0;34m(W, H_inv, num_bits, block_size)\u001b[0m\n\u001b[1;32m     20\u001b[0m Q \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(W)\n\u001b[1;32m     21\u001b[0m E \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(d_row, block_size, device\u001b[38;5;241m=\u001b[39mW\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 23\u001b[0m H_inv_chol \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH_inv\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, d_col, block_size):\n\u001b[1;32m     26\u001b[0m     curr_block_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(block_size, d_col \u001b[38;5;241m-\u001b[39m i)\n",
      "\u001b[0;31m_LinAlgError\u001b[0m: linalg.cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 43 is not positive-definite)."
     ]
    }
   ],
   "source": [
    "quantized_model = quantize_model(model, num_bits=8, method='gptq', block_size=128, lambda_param=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f638c63c-a0c2-4141-a446-bedcdd868478",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
